{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, mf_dim, layers, num_anime_features):\n",
    "        super(NCF, self).__init__()\n",
    "\n",
    "        # MF layers\n",
    "        self.user_embed_MF = nn.Embedding(num_users, mf_dim)\n",
    "        self.item_embed_MF = nn.Embedding(num_items, mf_dim)\n",
    "\n",
    "        # MLP layers\n",
    "        self.user_embed_MLP = nn.Embedding(num_users, layers[0] // 2)\n",
    "        self.item_embed_MLP = nn.Embedding(num_items, layers[0] // 2)\n",
    "        \n",
    "        # Anime features embedding layer\n",
    "        self.anime_feature_embed = nn.Linear(num_anime_features, layers[0] // 2)\n",
    "        \n",
    "        # self.mlp = nn.Sequential([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers) - 1)])\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(3 * (layers[0] // 2), 256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "\n",
    "        # final prediction layer\n",
    "        # self.prediction = nn.Linear(mf_dim + layers[-1], 1)\n",
    "        self.prediction = nn.Linear(40, 1)\n",
    "\n",
    "    def forward(self, user_indices, item_indices, anime_features):\n",
    "        # MF part\n",
    "        user_embed_MF = self.user_embed_MF(user_indices)\n",
    "        item_embed_MF = self.item_embed_MF(item_indices)\n",
    "        mf_vector = user_embed_MF * item_embed_MF\n",
    "\n",
    "        # MLP part\n",
    "        user_embed_MLP = self.user_embed_MLP(user_indices)\n",
    "        item_embed_MLP = self.item_embed_MLP(item_indices)\n",
    "        \n",
    "        # Anime features part\n",
    "        anime_feature_embed = self.anime_feature_embed(anime_features)\n",
    "        \n",
    "        # user_embed_MLP (layers[0] // 2)\n",
    "        # item_embed_MLP (layers[0] // 2)\n",
    "        # anime_feature_embed (layers[0] // 2)\n",
    "\n",
    "        mlp_vector = torch.cat([user_embed_MLP, item_embed_MLP, anime_feature_embed], dim=-1)\n",
    "        mlp_vector = self.mlp(mlp_vector)\n",
    "\n",
    "        # concatenate MF and MLP parts\n",
    "        vector = torch.cat([mf_vector, mlp_vector], dim=-1)\n",
    "\n",
    "        # final prediction\n",
    "        pred = self.prediction(vector)\n",
    "        return torch.sigmoid(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingDataset(Dataset):\n",
    "    \"\"\"Rating Dataset for DataLoader\"\"\"\n",
    "\n",
    "    def __init__(self, user_tensor, item_tensor, feature_tensor, target_tensor=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_tensor (torch.Tensor): User ID tensor. Size [n_samples]\n",
    "            item_tensor (torch.Tensor): Item ID tensor. Size [n_samples]\n",
    "            feature_tensor (torch.Tensor): Feature tensor. Size [n_samples, n_features]\n",
    "            target_tensor (torch.Tensor, optional): Target tensor. Size [n_samples]\n",
    "        \"\"\"\n",
    "        self.user_tensor = user_tensor\n",
    "        self.item_tensor = item_tensor\n",
    "        self.feature_tensor = feature_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.target_tensor is None:\n",
    "            return self.user_tensor[index], self.item_tensor[index], self.feature_tensor[index]\n",
    "        else:\n",
    "            return self.user_tensor[index], self.item_tensor[index], self.feature_tensor[index], self.target_tensor[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.user_tensor.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data\n",
    "train_data = pd.read_csv('../../data/train.csv')\n",
    "anime_data = pd.read_csv('../../data/anime.csv')\n",
    "test_data = pd.read_csv('../../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136401, 10)\n",
      "(2000, 18)\n",
      "(117676, 9)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(anime_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create anime_features\n",
    "anime_features = anime_data[['anime_id', 'episodes', 'members', 'watching', 'completed', 'on_hold', 'dropped', 'plan_to_watch']]\n",
    "\n",
    "# Merge train data and anime features\n",
    "train_data = train_data.merge(anime_features, on='anime_id', how='left')\n",
    "\n",
    "# Merge test data and anime features\n",
    "test_data = test_data.merge(anime_features, on='anime_id', how='left')\n",
    "\n",
    "# Get unique user and item IDs\n",
    "unique_user_ids = pd.unique(pd.concat([train_data['user_id'], test_data['user_id']]))\n",
    "unique_item_ids = pd.unique(pd.concat([train_data['anime_id'], test_data['anime_id']]))\n",
    "\n",
    "# Create user and item ID to index mapping\n",
    "user_id_to_index = {user_id: index for index, user_id in enumerate(unique_user_ids)}\n",
    "item_id_to_index = {item_id: index for index, item_id in enumerate(unique_item_ids)}\n",
    "\n",
    "# Apply mapping to train and test data\n",
    "train_data['user_id'] = train_data['user_id'].map(user_id_to_index)\n",
    "train_data['anime_id'] = train_data['anime_id'].map(item_id_to_index)\n",
    "test_data['user_id'] = test_data['user_id'].map(user_id_to_index)\n",
    "test_data['anime_id'] = test_data['anime_id'].map(item_id_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge train data and anime features\n",
    "# train_data = pd.read_csv('/mnt/data/train.csv')\n",
    "# train_data = train_data.merge(anime_features, on='anime_id', how='left')\n",
    "\n",
    "# # Merge test data and anime features\n",
    "# test_data = pd.read_csv('/mnt/data/test.csv')\n",
    "# test_data = test_data.merge(anime_features, on='anime_id', how='left')\n",
    "\n",
    "# # Get unique user and item IDs\n",
    "# unique_user_ids = pd.unique(pd.concat([train_data['user_id'], test_data['user_id']]))\n",
    "# unique_item_ids = pd.unique(pd.concat([train_data['anime_id'], test_data['anime_id']]))\n",
    "\n",
    "# # Create user and item ID to index mapping\n",
    "# user_id_to_index = {user_id: index for index, user_id in enumerate(unique_user_ids)}\n",
    "# item_id_to_index = {item_id: index for index, item_id in enumerate(unique_item_ids)}\n",
    "\n",
    "# # Apply mapping to train and test data\n",
    "# train_data['user_id'] = train_data['user_id'].map(user_id_to_index)\n",
    "# train_data['anime_id'] = train_data['anime_id'].map(item_id_to_index)\n",
    "# test_data['user_id'] = test_data['user_id'].map(user_id_to_index)\n",
    "# test_data['anime_id'] = test_data['anime_id'].map(item_id_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    1,    2,  ..., 1791, 1792, 1793])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_user_tensor.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    1,    2,  ..., 1995, 1996, 1997])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_user_tensor.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:03<00:31,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 62.727272033691406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:06<00:26,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 58.464115142822266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:10<00:23,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 63.7559814453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:13<00:19,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 62.90909194946289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:16<00:16,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 64.18659973144531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:19<00:13,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 62.483253479003906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:23<00:09,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 61.976078033447266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:26<00:06,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 61.784690856933594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:29<00:03,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 62.349281311035156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:33<00:00,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 63.47846984863281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m     predictions \u001b[39m=\u001b[39m []\n\u001b[1;32m     58\u001b[0m     \u001b[39mfor\u001b[39;00m user, item, feature \u001b[39min\u001b[39;00m test_loader:\n\u001b[0;32m---> 59\u001b[0m         outputs \u001b[39m=\u001b[39m model(user, item, feature)\n\u001b[1;32m     60\u001b[0m         predictions\u001b[39m.\u001b[39mextend(outputs\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mtolist())\n\u001b[1;32m     62\u001b[0m \u001b[39m# Print first 10 predictions\u001b[39;00m\n",
      "File \u001b[0;32m~/atmacup15/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[56], line 30\u001b[0m, in \u001b[0;36mNCF.forward\u001b[0;34m(self, user_indices, item_indices, anime_features)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, user_indices, item_indices, anime_features):\n\u001b[1;32m     29\u001b[0m     \u001b[39m# MF part\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     user_embed_MF \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muser_embed_MF(user_indices)\n\u001b[1;32m     31\u001b[0m     item_embed_MF \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitem_embed_MF(item_indices)\n\u001b[1;32m     32\u001b[0m     mf_vector \u001b[39m=\u001b[39m user_embed_MF \u001b[39m*\u001b[39m item_embed_MF\n",
      "File \u001b[0;32m~/atmacup15/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/atmacup15/.venv/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/atmacup15/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# Replace 'Unknown' with NaN in 'episodes' column\n",
    "train_data['episodes'] = train_data['episodes'].replace('Unknown', np.nan)\n",
    "test_data['episodes'] = test_data['episodes'].replace('Unknown', np.nan)\n",
    "\n",
    "# Convert 'episodes' column to float type\n",
    "train_data['episodes'] = train_data['episodes'].astype(float)\n",
    "test_data['episodes'] = test_data['episodes'].astype(float)\n",
    "\n",
    "# Fill NaN with the median of the column\n",
    "train_data['episodes'].fillna(train_data['episodes'].median(), inplace=True)\n",
    "test_data['episodes'].fillna(test_data['episodes'].median(), inplace=True)\n",
    "\n",
    "# Prepare training data\n",
    "train_user_tensor = torch.from_numpy(train_data['user_id'].values.astype(np.int64))\n",
    "train_item_tensor = torch.from_numpy(train_data['anime_id'].values.astype(np.int64))\n",
    "train_feature_tensor = torch.from_numpy(train_data[anime_features.columns].values.astype(np.float32))\n",
    "train_target_tensor = torch.from_numpy(train_data['score'].values.astype(np.float32))\n",
    "train_dataset = RatingDataset(train_user_tensor, train_item_tensor, train_feature_tensor, train_target_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "# Prepare test data\n",
    "test_user_tensor = torch.from_numpy(test_data['user_id'].values.astype(np.int64))\n",
    "test_item_tensor = torch.from_numpy(test_data['anime_id'].values.astype(np.int64))\n",
    "test_feature_tensor = torch.from_numpy(test_data[anime_features.columns].values.astype(np.float32))\n",
    "test_dataset = RatingDataset(test_user_tensor, test_item_tensor, test_feature_tensor, target_tensor=None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "num_users = len(train_data['user_id'].unique())\n",
    "num_items = len(train_data['anime_id'].unique())\n",
    "num_anime_features = len(anime_features.columns)\n",
    "mf_dim = 8  # dimension of MF\n",
    "layers = [384, 128, 32, 8]  # layer size of MLP\n",
    "model = NCF(num_users, num_items, mf_dim, layers, num_anime_features)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train model\n",
    "for epoch in tqdm(range(10)):  # run for 10 epochs\n",
    "    for user, item, feature, rating in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(user, item, feature)\n",
    "        loss = criterion(outputs.squeeze(), rating)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{10}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     predictions \u001b[39m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m     \u001b[39mfor\u001b[39;00m user, item, feature \u001b[39min\u001b[39;00m test_loader:\n\u001b[0;32m----> 6\u001b[0m         outputs \u001b[39m=\u001b[39m model(user, item, feature)\n\u001b[1;32m      7\u001b[0m         predictions\u001b[39m.\u001b[39mextend(outputs\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mtolist())\n\u001b[1;32m      9\u001b[0m \u001b[39m# Print first 10 predictions\u001b[39;00m\n",
      "File \u001b[0;32m~/atmacup15/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[56], line 30\u001b[0m, in \u001b[0;36mNCF.forward\u001b[0;34m(self, user_indices, item_indices, anime_features)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, user_indices, item_indices, anime_features):\n\u001b[1;32m     29\u001b[0m     \u001b[39m# MF part\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     user_embed_MF \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muser_embed_MF(user_indices)\n\u001b[1;32m     31\u001b[0m     item_embed_MF \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitem_embed_MF(item_indices)\n\u001b[1;32m     32\u001b[0m     mf_vector \u001b[39m=\u001b[39m user_embed_MF \u001b[39m*\u001b[39m item_embed_MF\n",
      "File \u001b[0;32m~/atmacup15/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/atmacup15/.venv/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/atmacup15/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# Make predictions on test set\n",
    "model.eval()  # switch to evaluation mode\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    for user, item, feature in test_loader:\n",
    "        outputs = model(user, item, feature)\n",
    "        predictions.extend(outputs.squeeze().tolist())\n",
    "\n",
    "# Print first 10 predictions\n",
    "print('First 10 predictions:', predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "384 // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [384, 192, 32, 8]\n",
    "[nn.Linear(layers[i], layers[i+1]) for i in range(len(layers) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_vector = torch.cat([user_embed_MLP, item_embed_MLP, anime_feature_embed], dim=-1)\n",
    "# ここの次元数が合わない"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
