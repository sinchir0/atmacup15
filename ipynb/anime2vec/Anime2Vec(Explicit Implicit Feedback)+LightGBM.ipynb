{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anime2Vec(Explicit/Implicit Feedback) + LightGBM\n",
    "\n",
    "[前回のnotebook](https://www.guruguru.science/competitions/21/discussions/57f5ea4e-69ad-439d-bbe3-887240cf5cf2/)ではレーティングの情報を活用する場合とそうでない場合を比較しましたが、今回はそれらを組み合わせることを考えます。\n",
    "\n",
    "[こちらのdiscussion](https://www.guruguru.science/competitions/21/discussions/d0e9e563-0910-46b9-a562-441b9c2bb843/)で紹介されているように、推薦システムにおけるユーザーからのフィードバックはExplicit FeedbackとImplicit Feedbackに分けることができます。  \n",
    "今回のAnime2Vecにおいて考えると、ユーザーレーティングを使用した場合はExplicit Feedbackをモデル化し、使用しない場合はある意味でImplicit Feedbackをモデル化していると考えることができます。  \n",
    "\n",
    "また、今回は`test.csv`が丸ごと与えられ、新規ユーザー（コールドユーザー）に対してもまとまった視聴情報、つまりImplicit Feedbackを得られているやや特殊な状況とも言えます。  \n",
    "折角なのでこれを活用する方法を考えたいところです。  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vecによる特徴量エンジニアリング(Explicit Feedback)\n",
    "\n",
    "こちらは[前回のnotebook](https://www.guruguru.science/competitions/21/discussions/57f5ea4e-69ad-439d-bbe3-887240cf5cf2/)と同様に、ユーザーのレーティングを考慮し、視聴回数分アニメを追加します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_w2v_features_with_score(train_df, val_df, test_df=None):\n",
    "    anime_ids = train_df['anime_id'].unique().tolist()\n",
    "    user_anime_list_dict = {user_id: anime_ids.tolist() for user_id, anime_ids in train_df.groupby('user_id')['anime_id']}\n",
    "\n",
    "    # スコアを考慮する場合\n",
    "    # 今回は1～10のレーティングなので、スコアが5のアニメは5回、スコアが10のアニメは10回、タイトルをリストに追加する\n",
    "    title_sentence_list = []\n",
    "    for user_id, user_df in train_df.groupby('user_id'):\n",
    "        user_title_sentence_list = []\n",
    "        for anime_id, anime_score in user_df[['anime_id', 'score']].values:\n",
    "            for i in range(anime_score):\n",
    "                user_title_sentence_list.append(anime_id)\n",
    "        title_sentence_list.append(user_title_sentence_list)\n",
    "\n",
    "    # ユーザごとにshuffleしたリストを作成\n",
    "    shuffled_sentence_list = [random.sample(sentence, len(sentence)) for sentence in title_sentence_list]  ## <= 変更点\n",
    "\n",
    "    # 元のリストとshuffleしたリストを合わせる\n",
    "    train_sentence_list = title_sentence_list + shuffled_sentence_list\n",
    "\n",
    "    # word2vecのパラメータ\n",
    "    vector_size = 64\n",
    "    w2v_params = {\n",
    "        \"vector_size\": vector_size,  ## <= 変更点\n",
    "        \"seed\": SEED,\n",
    "        \"min_count\": 1,\n",
    "        \"workers\": 1\n",
    "    }\n",
    "\n",
    "    # word2vecのモデル学習\n",
    "    model = word2vec.Word2Vec(train_sentence_list, **w2v_params)\n",
    "\n",
    "    # ユーザーごとの特徴ベクトルと対応するユーザーID\n",
    "    user_factors = {user_id: np.mean([model.wv[anime_id] for anime_id in user_anime_list], axis=0) for user_id, user_anime_list in user_anime_list_dict.items()}\n",
    "\n",
    "    # アイテムごとの特徴ベクトルと対応するアイテムID\n",
    "    item_factors = {aid: model.wv[aid] for aid in anime_ids}\n",
    "\n",
    "    # データフレームを作成\n",
    "    user_factors_df = pd.DataFrame(user_factors).T.reset_index().rename(columns={\"index\": \"user_id\"})\n",
    "    item_factors_df = pd.DataFrame(item_factors).T.reset_index().rename(columns={\"index\": \"anime_id\"})\n",
    "\n",
    "    # データフレームのカラム名をリネーム\n",
    "    user_factors_df.columns = [\"user_id\"] + [f\"user_factor_{i}\" for i in range(vector_size)]\n",
    "    item_factors_df.columns = [\"anime_id\"] + [f\"item_factor_{i}\" for i in range(vector_size)]\n",
    "\n",
    "    train_df = train_df.merge(user_factors_df, on=\"user_id\", how=\"left\")\n",
    "    train_df = train_df.merge(item_factors_df, on=\"anime_id\", how=\"left\")\n",
    "\n",
    "    val_df = val_df.merge(user_factors_df, on=\"user_id\", how=\"left\")\n",
    "    val_df = val_df.merge(item_factors_df, on=\"anime_id\", how=\"left\")\n",
    "\n",
    "    if test_df is not None:\n",
    "        test_df = test_df.merge(user_factors_df, on=\"user_id\", how=\"left\")\n",
    "        test_df = test_df.merge(item_factors_df, on=\"anime_id\", how=\"left\")\n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "    return train_df, val_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vecによる特徴量エンジニアリング(Implicit Feedback)\n",
    "\n",
    "今回は`test.csv`が丸ごと与えられ、新規ユーザーに対しても視聴情報が得られる状況なのでこれを活用します。  \n",
    "とは言っても、単にtrain/testを先に結合してからスコアの情報を使用せずにWord2Vecの学習を行うだけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_w2v_features_without_score(train_test_df):\n",
    "    \n",
    "    anime_ids = train_test_df['anime_id'].unique().tolist()\n",
    "    user_anime_list_dict = {user_id: anime_ids.tolist() for user_id, anime_ids in train_test_df.groupby('user_id')['anime_id']}\n",
    "\n",
    "    title_sentence_list = train_test_df.groupby('user_id')['anime_id'].apply(list).tolist()\n",
    "\n",
    "    # ユーザごとにshuffleしたリストを作成\n",
    "    shuffled_sentence_list = [random.sample(sentence, len(sentence)) for sentence in title_sentence_list]  ## <= 変更点\n",
    "\n",
    "    # 元のリストとshuffleしたリストを合わせる\n",
    "    train_sentence_list = title_sentence_list + shuffled_sentence_list\n",
    "    print(len(train_sentence_list))\n",
    "    # word2vecのパラメータ\n",
    "    vector_size = 64\n",
    "    w2v_params = {\n",
    "        \"vector_size\": vector_size,  ## <= 変更点\n",
    "        \"seed\": SEED,\n",
    "        \"min_count\": 1,\n",
    "        \"workers\": 1\n",
    "    }\n",
    "\n",
    "    # word2vecのモデル学習\n",
    "    model = word2vec.Word2Vec(train_sentence_list, **w2v_params)\n",
    "\n",
    "    # ユーザーごとの特徴ベクトルと対応するユーザーID\n",
    "    user_factors = {user_id: np.mean([model.wv[anime_id] for anime_id in user_anime_list], axis=0) for user_id, user_anime_list in user_anime_list_dict.items()}\n",
    "\n",
    "    # アイテムごとの特徴ベクトルと対応するアイテムID\n",
    "    item_factors = {aid: model.wv[aid] for aid in anime_ids}\n",
    "\n",
    "    # データフレームを作成\n",
    "    user_factors_df = pd.DataFrame(user_factors).T.reset_index().rename(columns={\"index\": \"user_id\"})\n",
    "    item_factors_df = pd.DataFrame(item_factors).T.reset_index().rename(columns={\"index\": \"anime_id\"})\n",
    "\n",
    "    # データフレームのカラム名をリネーム\n",
    "    user_factors_df.columns = [\"user_id\"] + [f\"wo_score_user_factor_{i}\" for i in range(vector_size)]\n",
    "    item_factors_df.columns = [\"anime_id\"] + [f\"wo_score_item_factor_{i}\" for i in range(vector_size)]\n",
    "\n",
    "    train_test_df = train_test_df.merge(user_factors_df, on=\"user_id\", how=\"left\")\n",
    "\n",
    "    return train_test_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習に便利な関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_df = pd.read_csv('../../data/train.csv')\n",
    "    test_df = pd.read_csv('../../data/test.csv')\n",
    "    test_df['score'] = 0 # dummy\n",
    "\n",
    "    # Initialize submission file\n",
    "    submission_df = pd.read_csv('../../data/sample_submission.csv')\n",
    "    submission_df['score'] = 0\n",
    "    return train_df, test_df, submission_df\n",
    "\n",
    "def stratified_and_group_kfold_split(train_df):\n",
    "    # https://www.guruguru.science/competitions/21/discussions/45ffc8a1-e37c-4b95-aac4-c4e338aa6a9b/\n",
    "    \n",
    "    # 20%のユーザを抽出\n",
    "    n_user = train_df[\"user_id\"].nunique()\n",
    "    unseen_users = random.sample(sorted(train_df[\"user_id\"].unique()), k=n_user // 5)\n",
    "    train_df[\"unseen_user\"] = train_df[\"user_id\"].isin(unseen_users)\n",
    "    unseen_df = train_df[train_df[\"unseen_user\"]].reset_index(drop=True)\n",
    "    train_df = train_df[~train_df[\"unseen_user\"]].reset_index(drop=True)\n",
    "\n",
    "    # train_dfの80%をStratifiedKFoldで分割\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    for fold_id, (_, valid_idx) in enumerate(skf.split(train_df, train_df[\"user_id\"])):\n",
    "        train_df.loc[valid_idx, \"fold\"] = fold_id\n",
    "\n",
    "    # 20%をGroupKFoldで分割\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    unseen_df[\"fold\"] = -1\n",
    "    for fold_id, (_, valid_idx) in enumerate(gkf.split(unseen_df, unseen_df[\"user_id\"], unseen_df[\"user_id\"])):\n",
    "        unseen_df.loc[valid_idx, \"fold\"] = fold_id\n",
    "\n",
    "    # concat\n",
    "    train_df = pd.concat([train_df, unseen_df], axis=0).reset_index(drop=True)\n",
    "    train_df.drop(columns=[\"unseen_user\"], inplace=True)\n",
    "    return train_df\n",
    "\n",
    "\n",
    "def train(train_df, original_test_df, submission_df):\n",
    "    train_df['oof'] = 0\n",
    "    train_df['seen'] = False\n",
    "\n",
    "    for fold in range(5):\n",
    "        # Prepare the train and validation data\n",
    "        trn_df = train_df[train_df['fold'] != fold].copy()\n",
    "        val_df = train_df[train_df['fold'] == fold].copy()\n",
    "\n",
    "        trn_df, val_df, test_df = add_w2v_features_with_score(trn_df, val_df, original_test_df.copy())\n",
    "        \n",
    "        # Define the features and the target\n",
    "        unused_cols = ['user_id', 'anime_id', 'score', 'fold', 'oof', 'seen']\n",
    "        feature_cols = [col for col in trn_df.columns if col not in unused_cols]\n",
    "        target_col = 'score'\n",
    "\n",
    "        # Prepare the LightGBM datasets\n",
    "        lgb_train = lgb.Dataset(trn_df[feature_cols], trn_df[target_col])\n",
    "        lgb_val = lgb.Dataset(val_df[feature_cols], val_df[target_col])\n",
    "\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'learning_rate': 0.1,\n",
    "            # 'reg_lambda': 1.0\n",
    "        }\n",
    "\n",
    "        # Train the model\n",
    "        callbacks = [\n",
    "            lgb.early_stopping(stopping_rounds=100),\n",
    "            lgb.log_evaluation(period=100)\n",
    "        ]\n",
    "        model_lgb = lgb.train(params, lgb_train, valid_sets=[\n",
    "                              lgb_train, lgb_val], callbacks=callbacks, num_boost_round=10000)\n",
    "\n",
    "        # Predict\n",
    "        trn_df['preds'] = model_lgb.predict(trn_df[feature_cols], num_iteration=model_lgb.best_iteration)\n",
    "        val_df['preds'] = model_lgb.predict(val_df[feature_cols], num_iteration=model_lgb.best_iteration)\n",
    "        test_preds = model_lgb.predict(test_df[feature_cols], num_iteration=model_lgb.best_iteration)\n",
    "\n",
    "        train_users = trn_df['user_id'].unique()\n",
    "        is_seen = val_df['user_id'].isin(train_users)\n",
    "        seen_val = val_df[is_seen]\n",
    "        unseen_val = val_df[~is_seen]\n",
    "\n",
    "        # Evaluate the model\n",
    "        train_score = np.sqrt(mean_squared_error(trn_df['score'], trn_df['preds']))\n",
    "        seen_val_score = np.sqrt(mean_squared_error(seen_val['score'], seen_val['preds']))\n",
    "        unseen_val_score = np.sqrt(mean_squared_error(unseen_val['score'], unseen_val['preds']))\n",
    "        print(f'fold{fold} train RMSE: {train_score:.3f}, seen val RMSE: {seen_val_score:.3f}, unseen val RMSE: {unseen_val_score:.3f}')\n",
    "        \n",
    "        submission_df['score'] += test_preds / 5\n",
    "\n",
    "        train_df.loc[train_df['fold'] == fold, 'oof'] = val_df['preds'].values\n",
    "        train_df.loc[train_df['fold'] == fold, 'seen'] = is_seen.values\n",
    "\n",
    "    total_score = np.sqrt(mean_squared_error(train_df['score'], train_df['oof']))\n",
    "    seen_score = np.sqrt(mean_squared_error(train_df[train_df['seen']]['score'], train_df[train_df['seen']]['oof']))\n",
    "    unseen_score = np.sqrt(mean_squared_error(train_df[~train_df['seen']]['score'], train_df[~train_df['seen']]['oof']))\n",
    "    print(f\"Total RMSE: {total_score} | Seen RMSE: {seen_score} | Unseen RMSE: {unseen_score}\")\n",
    "\n",
    "    submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df, submission_df = load_data()\n",
    "\n",
    "train_test_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "train_test_df = add_w2v_features_without_score(train_test_df)\n",
    "add_w2v_features_with_score(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load the data] done in 0 s\n",
      "[Stratified & Group split] done in 0 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shinichiro.saito/atmacup15/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[add_w2v_features_without_score] done in 2 s\n"
     ]
    }
   ],
   "source": [
    "# with timer(\"Load the data\"):\n",
    "#     train_df, test_df, submission_df = load_data()\n",
    "\n",
    "# with timer(\"Stratified & Group split\"):\n",
    "#     train_df = stratified_and_group_kfold_split(train_df)\n",
    "\n",
    "# with timer(\"add_w2v_features_without_score\"):\n",
    "#     # testの視聴情報も活用するため、trainとtestを結合して先に特徴量を作成\n",
    "#     train_test_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n",
    "#     train_test_df = add_w2v_features_without_score(train_test_df)\n",
    "# #     train_df = train_test_df[train_test_df['score'] != 0].copy().reset_index(drop=True)\n",
    "# #     test_df = train_test_df[train_test_df['score'] == 0].copy().reset_index(drop=True)\n",
    "\n",
    "# # with timer(\"Training and evaluation with LightGBM\"):\n",
    "# #     train(train_df, test_df, submission_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>score</th>\n",
       "      <th>fold</th>\n",
       "      <th>wo_score_user_factor_0</th>\n",
       "      <th>wo_score_user_factor_1</th>\n",
       "      <th>wo_score_user_factor_2</th>\n",
       "      <th>wo_score_user_factor_3</th>\n",
       "      <th>wo_score_user_factor_4</th>\n",
       "      <th>wo_score_user_factor_5</th>\n",
       "      <th>...</th>\n",
       "      <th>wo_score_user_factor_54</th>\n",
       "      <th>wo_score_user_factor_55</th>\n",
       "      <th>wo_score_user_factor_56</th>\n",
       "      <th>wo_score_user_factor_57</th>\n",
       "      <th>wo_score_user_factor_58</th>\n",
       "      <th>wo_score_user_factor_59</th>\n",
       "      <th>wo_score_user_factor_60</th>\n",
       "      <th>wo_score_user_factor_61</th>\n",
       "      <th>wo_score_user_factor_62</th>\n",
       "      <th>wo_score_user_factor_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001a7aed2546342e2602</td>\n",
       "      <td>034eb6feb083d80751a4</td>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.207528</td>\n",
       "      <td>-0.130969</td>\n",
       "      <td>0.102914</td>\n",
       "      <td>-0.177719</td>\n",
       "      <td>-0.099937</td>\n",
       "      <td>-0.208528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023294</td>\n",
       "      <td>0.204172</td>\n",
       "      <td>-0.021628</td>\n",
       "      <td>0.084894</td>\n",
       "      <td>0.025680</td>\n",
       "      <td>0.123446</td>\n",
       "      <td>0.351074</td>\n",
       "      <td>-0.147088</td>\n",
       "      <td>-0.232003</td>\n",
       "      <td>0.165780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001a7aed2546342e2602</td>\n",
       "      <td>04068820a73e52dc3b32</td>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.207528</td>\n",
       "      <td>-0.130969</td>\n",
       "      <td>0.102914</td>\n",
       "      <td>-0.177719</td>\n",
       "      <td>-0.099937</td>\n",
       "      <td>-0.208528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023294</td>\n",
       "      <td>0.204172</td>\n",
       "      <td>-0.021628</td>\n",
       "      <td>0.084894</td>\n",
       "      <td>0.025680</td>\n",
       "      <td>0.123446</td>\n",
       "      <td>0.351074</td>\n",
       "      <td>-0.147088</td>\n",
       "      <td>-0.232003</td>\n",
       "      <td>0.165780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001a7aed2546342e2602</td>\n",
       "      <td>057c8610088179f68964</td>\n",
       "      <td>9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.207528</td>\n",
       "      <td>-0.130969</td>\n",
       "      <td>0.102914</td>\n",
       "      <td>-0.177719</td>\n",
       "      <td>-0.099937</td>\n",
       "      <td>-0.208528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023294</td>\n",
       "      <td>0.204172</td>\n",
       "      <td>-0.021628</td>\n",
       "      <td>0.084894</td>\n",
       "      <td>0.025680</td>\n",
       "      <td>0.123446</td>\n",
       "      <td>0.351074</td>\n",
       "      <td>-0.147088</td>\n",
       "      <td>-0.232003</td>\n",
       "      <td>0.165780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001a7aed2546342e2602</td>\n",
       "      <td>08aaefd0726338c6cda6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.207528</td>\n",
       "      <td>-0.130969</td>\n",
       "      <td>0.102914</td>\n",
       "      <td>-0.177719</td>\n",
       "      <td>-0.099937</td>\n",
       "      <td>-0.208528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023294</td>\n",
       "      <td>0.204172</td>\n",
       "      <td>-0.021628</td>\n",
       "      <td>0.084894</td>\n",
       "      <td>0.025680</td>\n",
       "      <td>0.123446</td>\n",
       "      <td>0.351074</td>\n",
       "      <td>-0.147088</td>\n",
       "      <td>-0.232003</td>\n",
       "      <td>0.165780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001a7aed2546342e2602</td>\n",
       "      <td>09d9688ffb425b3903b2</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.207528</td>\n",
       "      <td>-0.130969</td>\n",
       "      <td>0.102914</td>\n",
       "      <td>-0.177719</td>\n",
       "      <td>-0.099937</td>\n",
       "      <td>-0.208528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023294</td>\n",
       "      <td>0.204172</td>\n",
       "      <td>-0.021628</td>\n",
       "      <td>0.084894</td>\n",
       "      <td>0.025680</td>\n",
       "      <td>0.123446</td>\n",
       "      <td>0.351074</td>\n",
       "      <td>-0.147088</td>\n",
       "      <td>-0.232003</td>\n",
       "      <td>0.165780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254072</th>\n",
       "      <td>ffe85a36cd20500faa58</td>\n",
       "      <td>f508b02efeac8ecb8cc0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.234619</td>\n",
       "      <td>-0.268356</td>\n",
       "      <td>0.334104</td>\n",
       "      <td>-0.424140</td>\n",
       "      <td>-0.196889</td>\n",
       "      <td>-0.348174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.597357</td>\n",
       "      <td>0.198136</td>\n",
       "      <td>0.352581</td>\n",
       "      <td>-0.014029</td>\n",
       "      <td>0.214613</td>\n",
       "      <td>0.124050</td>\n",
       "      <td>0.399124</td>\n",
       "      <td>0.054557</td>\n",
       "      <td>-0.253500</td>\n",
       "      <td>0.196781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254073</th>\n",
       "      <td>ffe85a36cd20500faa58</td>\n",
       "      <td>f5b8ecea3beea4b82d79</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.234619</td>\n",
       "      <td>-0.268356</td>\n",
       "      <td>0.334104</td>\n",
       "      <td>-0.424140</td>\n",
       "      <td>-0.196889</td>\n",
       "      <td>-0.348174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.597357</td>\n",
       "      <td>0.198136</td>\n",
       "      <td>0.352581</td>\n",
       "      <td>-0.014029</td>\n",
       "      <td>0.214613</td>\n",
       "      <td>0.124050</td>\n",
       "      <td>0.399124</td>\n",
       "      <td>0.054557</td>\n",
       "      <td>-0.253500</td>\n",
       "      <td>0.196781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254074</th>\n",
       "      <td>ffe85a36cd20500faa58</td>\n",
       "      <td>f6c208226b6b69948053</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.234619</td>\n",
       "      <td>-0.268356</td>\n",
       "      <td>0.334104</td>\n",
       "      <td>-0.424140</td>\n",
       "      <td>-0.196889</td>\n",
       "      <td>-0.348174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.597357</td>\n",
       "      <td>0.198136</td>\n",
       "      <td>0.352581</td>\n",
       "      <td>-0.014029</td>\n",
       "      <td>0.214613</td>\n",
       "      <td>0.124050</td>\n",
       "      <td>0.399124</td>\n",
       "      <td>0.054557</td>\n",
       "      <td>-0.253500</td>\n",
       "      <td>0.196781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254075</th>\n",
       "      <td>ffe85a36cd20500faa58</td>\n",
       "      <td>fe67592c312fc1e17745</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.234619</td>\n",
       "      <td>-0.268356</td>\n",
       "      <td>0.334104</td>\n",
       "      <td>-0.424140</td>\n",
       "      <td>-0.196889</td>\n",
       "      <td>-0.348174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.597357</td>\n",
       "      <td>0.198136</td>\n",
       "      <td>0.352581</td>\n",
       "      <td>-0.014029</td>\n",
       "      <td>0.214613</td>\n",
       "      <td>0.124050</td>\n",
       "      <td>0.399124</td>\n",
       "      <td>0.054557</td>\n",
       "      <td>-0.253500</td>\n",
       "      <td>0.196781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254076</th>\n",
       "      <td>ffe85a36cd20500faa58</td>\n",
       "      <td>ff73475b68001c5e533d</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.234619</td>\n",
       "      <td>-0.268356</td>\n",
       "      <td>0.334104</td>\n",
       "      <td>-0.424140</td>\n",
       "      <td>-0.196889</td>\n",
       "      <td>-0.348174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.597357</td>\n",
       "      <td>0.198136</td>\n",
       "      <td>0.352581</td>\n",
       "      <td>-0.014029</td>\n",
       "      <td>0.214613</td>\n",
       "      <td>0.124050</td>\n",
       "      <td>0.399124</td>\n",
       "      <td>0.054557</td>\n",
       "      <td>-0.253500</td>\n",
       "      <td>0.196781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254077 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     user_id              anime_id  score  fold  \\\n",
       "0       001a7aed2546342e2602  034eb6feb083d80751a4      9   3.0   \n",
       "1       001a7aed2546342e2602  04068820a73e52dc3b32      9   3.0   \n",
       "2       001a7aed2546342e2602  057c8610088179f68964      9   4.0   \n",
       "3       001a7aed2546342e2602  08aaefd0726338c6cda6      8   0.0   \n",
       "4       001a7aed2546342e2602  09d9688ffb425b3903b2      8   1.0   \n",
       "...                      ...                   ...    ...   ...   \n",
       "254072  ffe85a36cd20500faa58  f508b02efeac8ecb8cc0      0   NaN   \n",
       "254073  ffe85a36cd20500faa58  f5b8ecea3beea4b82d79      0   NaN   \n",
       "254074  ffe85a36cd20500faa58  f6c208226b6b69948053      0   NaN   \n",
       "254075  ffe85a36cd20500faa58  fe67592c312fc1e17745      0   NaN   \n",
       "254076  ffe85a36cd20500faa58  ff73475b68001c5e533d      0   NaN   \n",
       "\n",
       "        wo_score_user_factor_0  wo_score_user_factor_1  \\\n",
       "0                     0.207528               -0.130969   \n",
       "1                     0.207528               -0.130969   \n",
       "2                     0.207528               -0.130969   \n",
       "3                     0.207528               -0.130969   \n",
       "4                     0.207528               -0.130969   \n",
       "...                        ...                     ...   \n",
       "254072                0.234619               -0.268356   \n",
       "254073                0.234619               -0.268356   \n",
       "254074                0.234619               -0.268356   \n",
       "254075                0.234619               -0.268356   \n",
       "254076                0.234619               -0.268356   \n",
       "\n",
       "        wo_score_user_factor_2  wo_score_user_factor_3  \\\n",
       "0                     0.102914               -0.177719   \n",
       "1                     0.102914               -0.177719   \n",
       "2                     0.102914               -0.177719   \n",
       "3                     0.102914               -0.177719   \n",
       "4                     0.102914               -0.177719   \n",
       "...                        ...                     ...   \n",
       "254072                0.334104               -0.424140   \n",
       "254073                0.334104               -0.424140   \n",
       "254074                0.334104               -0.424140   \n",
       "254075                0.334104               -0.424140   \n",
       "254076                0.334104               -0.424140   \n",
       "\n",
       "        wo_score_user_factor_4  wo_score_user_factor_5  ...  \\\n",
       "0                    -0.099937               -0.208528  ...   \n",
       "1                    -0.099937               -0.208528  ...   \n",
       "2                    -0.099937               -0.208528  ...   \n",
       "3                    -0.099937               -0.208528  ...   \n",
       "4                    -0.099937               -0.208528  ...   \n",
       "...                        ...                     ...  ...   \n",
       "254072               -0.196889               -0.348174  ...   \n",
       "254073               -0.196889               -0.348174  ...   \n",
       "254074               -0.196889               -0.348174  ...   \n",
       "254075               -0.196889               -0.348174  ...   \n",
       "254076               -0.196889               -0.348174  ...   \n",
       "\n",
       "        wo_score_user_factor_54  wo_score_user_factor_55  \\\n",
       "0                     -0.023294                 0.204172   \n",
       "1                     -0.023294                 0.204172   \n",
       "2                     -0.023294                 0.204172   \n",
       "3                     -0.023294                 0.204172   \n",
       "4                     -0.023294                 0.204172   \n",
       "...                         ...                      ...   \n",
       "254072                -0.597357                 0.198136   \n",
       "254073                -0.597357                 0.198136   \n",
       "254074                -0.597357                 0.198136   \n",
       "254075                -0.597357                 0.198136   \n",
       "254076                -0.597357                 0.198136   \n",
       "\n",
       "        wo_score_user_factor_56  wo_score_user_factor_57  \\\n",
       "0                     -0.021628                 0.084894   \n",
       "1                     -0.021628                 0.084894   \n",
       "2                     -0.021628                 0.084894   \n",
       "3                     -0.021628                 0.084894   \n",
       "4                     -0.021628                 0.084894   \n",
       "...                         ...                      ...   \n",
       "254072                 0.352581                -0.014029   \n",
       "254073                 0.352581                -0.014029   \n",
       "254074                 0.352581                -0.014029   \n",
       "254075                 0.352581                -0.014029   \n",
       "254076                 0.352581                -0.014029   \n",
       "\n",
       "        wo_score_user_factor_58  wo_score_user_factor_59  \\\n",
       "0                      0.025680                 0.123446   \n",
       "1                      0.025680                 0.123446   \n",
       "2                      0.025680                 0.123446   \n",
       "3                      0.025680                 0.123446   \n",
       "4                      0.025680                 0.123446   \n",
       "...                         ...                      ...   \n",
       "254072                 0.214613                 0.124050   \n",
       "254073                 0.214613                 0.124050   \n",
       "254074                 0.214613                 0.124050   \n",
       "254075                 0.214613                 0.124050   \n",
       "254076                 0.214613                 0.124050   \n",
       "\n",
       "        wo_score_user_factor_60  wo_score_user_factor_61  \\\n",
       "0                      0.351074                -0.147088   \n",
       "1                      0.351074                -0.147088   \n",
       "2                      0.351074                -0.147088   \n",
       "3                      0.351074                -0.147088   \n",
       "4                      0.351074                -0.147088   \n",
       "...                         ...                      ...   \n",
       "254072                 0.399124                 0.054557   \n",
       "254073                 0.399124                 0.054557   \n",
       "254074                 0.399124                 0.054557   \n",
       "254075                 0.399124                 0.054557   \n",
       "254076                 0.399124                 0.054557   \n",
       "\n",
       "        wo_score_user_factor_62  wo_score_user_factor_63  \n",
       "0                     -0.232003                 0.165780  \n",
       "1                     -0.232003                 0.165780  \n",
       "2                     -0.232003                 0.165780  \n",
       "3                     -0.232003                 0.165780  \n",
       "4                     -0.232003                 0.165780  \n",
       "...                         ...                      ...  \n",
       "254072                -0.253500                 0.196781  \n",
       "254073                -0.253500                 0.196781  \n",
       "254074                -0.253500                 0.196781  \n",
       "254075                -0.253500                 0.196781  \n",
       "254076                -0.253500                 0.196781  \n",
       "\n",
       "[254077 rows x 68 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total RMSE: 1.2404953346081091  \n",
    "Seen RMSE: 1.1849511964814186  \n",
    "Unseen RMSE: 1.43856427010653  \n",
    "となりました。  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### コールドユーザーに対するナイーブな予測（おまけ）\n",
    "さて、上記のunseen userに対するスコアRMSE=1.438というのはどの程度いいのでしょうか。  \n",
    "単に未知ユーザーに対してはアニメ全体の予測値の平均で埋めるナイーブな手法と比較してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4414882404447242\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# データを読み込む\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "# ユーザーごとにデータを分割\n",
    "users = train['user_id'].unique()\n",
    "\n",
    "# ユーザーIDを訓練セットと検証セットに分割\n",
    "train_users, val_users = train_test_split(users, test_size=0.2, random_state=42)\n",
    "\n",
    "# 訓練セットと検証セットを作成\n",
    "train_data = train[train['user_id'].isin(train_users)]\n",
    "val_data = train[train['user_id'].isin(val_users)]\n",
    "\n",
    "# アニメの平均スコアを計算\n",
    "anime_mean_score = train_data.groupby('anime_id')['score'].mean()\n",
    "\n",
    "# 検証データに対する予測値を計算\n",
    "val_data = val_data.copy()  # Avoid SettingWithCopyWarning\n",
    "val_data['pred_score'] = val_data['anime_id'].map(anime_mean_score)\n",
    "\n",
    "# 訓練データ全体の平均スコアを計算\n",
    "global_mean_score = train_data['score'].mean()\n",
    "\n",
    "# NaNを訓練データ全体の平均スコアで補完\n",
    "val_data['pred_score'].fillna(global_mean_score, inplace=True)\n",
    "\n",
    "# 再度、ベースラインモデルのRMSEを計算\n",
    "rmse = np.sqrt(mean_squared_error(val_data['score'], val_data['pred_score']))\n",
    "\n",
    "print(rmse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・・・あまり変わらないような気も？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
